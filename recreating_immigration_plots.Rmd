---
title: "Recreating Immigration Plots"
author: "Shimmy Greengart, Smriti Karki"
date: "2024-06-24"
output:
    html_document:
        toc: yes
        code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(modelsummary)
library(modelr)
```

# Initial Replication Study

## Figure 4

First, we get the data. We downloaded it fresh from [Google Trends](https://trends.google.com/trends/explore?date=2004-01-01%202019-12-31&geo=US&q=report%20immigrant%20%2B%20report%20immigration%20%2B%20report%20illegals%20%2B%20report%20illegal%20alien%20%2B%20report%20to%20ice,immigrant%20crime%20%2B%20immigrant%20criminal%20%2B%20immigrant%20murder%20%2B%20immigrant%20kill,immigrant%20welfare%20%2B%20immigrant%20cost%20%2B%20immigrant%20benefits&hl=en), looking at all 3 together in comparison. 

```{r}
immigration_data <- read_csv('from_google_trends/google_trends_collective.csv', skip=3, col_names=c('month', 'welfare_trend', 'crime_trend', 'report_trend')) |>
    mutate(month = parse_date(month, "%Y-%m")) |>
    mutate(president=as.factor(ifelse(year(month) <= 2008, "Bush", ifelse(year(month) <= 2016, "Obama", "Trump"))))

```
Now, we recreate the plot.

```{r}
immigration_data |>
    pivot_longer(names_to = "trend_type", values_to = "trend_value", cols = ends_with("_trend")) |>
    mutate(trend_type=factor(trend_type, labels=c("report_trend", "crime_trend", "welfare_trend"))) |>
    ggplot(aes(x=month, y=trend_value, color=president)) +
    geom_point(alpha = 0.3) +
    geom_smooth(method=lm, formula = y ~ x, se=F) +
    facet_wrap(~ trend_type, dir="v")
```
## Table 3

Now we replicate table 3.

```{r}
table_3_data <- immigration_data |>
    mutate(is_bush = president == "Bush", is_trump = president == "Trump")

models <- list(
    crime = lm(crime_trend ~ month + is_bush + is_trump, data = table_3_data),
    welfare = lm(welfare_trend ~ month + is_bush + is_trump, data = table_3_data),
    report = lm(report_trend ~ month + is_bush + is_trump, data = table_3_data)
)

msummary(models, output='markdown')
```
We are getting different values for the model. Not only are the constants off, but they have welfare increasing by date, while we have it decreasing. They also have big positive changes from Bush. We have small positive changes from Crime and Welfare, and by Reporting, where they have a small positive change, we have a small negative change. And they find bigger changes for Trump than we do.

But, this still supports their point from this table, which was that Trump's election led to a large positive increase in all these searches, which we do find.

```{r}
table_3_data |>
    add_predictions(models[["report"]]) |>
    ggplot(aes(x=month, y=report_trend, color=president)) +
    geom_point(alpha = 0.3) +
    geom_smooth(method=lm, formula = y ~ x, se=F) +
    geom_line(aes(y=pred), linetype="dashed")

table_3_data |>
    add_predictions(models[["crime"]]) |>
    ggplot(aes(x=month, y=crime_trend, color=president)) +
    geom_point(alpha = 0.3) +
    geom_smooth(method=lm, formula = y ~ x, se=F) +
    geom_line(aes(y=pred), linetype="dashed")

table_3_data |>
    add_predictions(models[["welfare"]]) |>
    ggplot(aes(x=month, y=welfare_trend, color=president)) +
    geom_point(alpha = 0.3) +
    geom_smooth(method=lm, formula = y ~ x, se=F) +
    geom_line(aes(y=pred), linetype="dashed")
```
This looks very different from out original plot, but that makes sense. Before, geom_smooth was using * to have different slopes for the different presidencies. Since here, we only have a constant based on who was president, we have one slope for everything.

## Figures 2 and 3

Now, we get the topic model data. The topic model was given to us by Jake Hofman of Microsoft Research. The model is not included in the repository because it was too big to fit. Instead, we have two matrices from the model, one containing the topics in each document and one containing each term's topic distribution.

```{r}
load('from_replication_files/topic_model_lite.RData')

document_topics <- document_topics |>
    mutate(date = ymd(date))
```

First, we make figure 2. It is about immigration coverage in general, not of a specific category, so I think all the documents there count.

```{r}
campaign_start <- document_topics |>
    filter(time == "pre-election") |>
    arrange(date) |>
    slice_tail(n=1) |>
    pull(date)

campaign_end <- document_topics |>
    filter(time == "post-election") |>
    arrange(date) |>
    slice_head(n=1) |>
    pull(date)

document_topics |>
    mutate(month = floor_date(date, "month")) |>
    group_by(month, channel, time) |>
    summarize(num_segments = n()) |>
    ggplot(aes(x=month, y=num_segments, color=channel)) +
    geom_point() +
    geom_smooth(aes(group = interaction(channel, time)), se=F) +
    geom_vline(aes(xintercept=campaign_start), linetype="dashed") +
    geom_vline(aes(xintercept=campaign_end), linetype="dashed") +
    scale_color_manual(values=c("magenta", "red", "blue"))

    
```
We have essentially the same results as they do, but with a less severe jump after the campaign start and a bigger drop before the inauguration. But their overall findings still show.

```{r}
document_topics |>
    mutate(month = floor_date(date, "month")) |>
    group_by(month, channel, time) |>
    summarize(proportion_1 = sum(Topic1), proportion_3 = sum(Topic3), crime_coverage = proportion_1 + proportion_3) |>
    ggplot(aes(x=month, y=crime_coverage, color=channel)) +
    geom_point() +
    geom_smooth(aes(group = interaction(channel, time)), se=F) +
    geom_vline(aes(xintercept=campaign_start), linetype="dashed") +
    geom_vline(aes(xintercept=campaign_end), linetype="dashed") +
    scale_color_manual(values=c("magenta", "red", "blue"))

document_topics |>
    mutate(month = floor_date(date, "month")) |>
    group_by(month, channel, time) |>
    summarize(welfare_coverage = sum(Topic13)) |>
    ggplot(aes(x=month, y=welfare_coverage, color=channel)) +
    geom_point() +
    geom_smooth(aes(group = interaction(channel, time)), se=F) +
    geom_vline(aes(xintercept=campaign_start), linetype="dashed") +
    geom_vline(aes(xintercept=campaign_end), linetype="dashed") +
    scale_color_manual(values=c("magenta", "red", "blue"))
```
This time, we get identical results. I suspect that in the original, we only differ because of how we deal with the months bifrucated by the dotted lines. We were consistent, always treating them as multiple points, causing the lines to cross the dotted lines. They only seem to have done that for the second one for some reason.

## Table 4 Monthly

Now we reproduce table 4. This is before we get the daily data, so we do it on a monthly level.

```{r}
topic_table <- document_topics |>
    mutate(month = floor_date(date, "month")) |>
    group_by(month, time) |>
    summarize(proportion_1 = sum(Topic1), proportion_3 = sum(Topic3), crime_coverage = proportion_1 + proportion_3, welfare_coverage = sum(Topic13), num_segments = n()) |>
    mutate(trump_admin = time == "post-election", month_of_year=month(month, label=T)) |>
    inner_join(immigration_data, by="month")

lm(report_trend ~ num_segments + crime_coverage + welfare_coverage + trump_admin + month + month_of_year, topic_table) |>
    msummary(output='markdown')
```
Obviously, this is different from their result, because it is on a monthly level. But we found significantly lower effects of number of crime and welfare coverage, and no effect from the number of segments. These things shouldn't change, but perhaps the effect only exists in the short-term (but if so, that changes the paper's conclusions). Though we did still have a high modifier for Trump's presidency, presumably because that was a long-term effect.

## Figure 4 and Table 3 Using Replication Files

We want to recreate figure 4 and table 3 using the data they used to see if we still get the same results. This data was supplied again by Jake Hofman, but we have included it for anyone trying to replicate this.

```{r}
report_data <- read_csv('from_replication_files/google_trends_report.csv') |>
    rename(report_trend = search)
crime_data <- read_csv('from_replication_files/google_trends_crime.csv') |>
    rename(crime_trend = search)
welfare_data <- read_csv('from_replication_files/google_trends_welfare.csv') |>
    rename(welfare_trend = search)

trend_data_given <- report_data |>
    inner_join(crime_data, by=c("year", "month")) |>
    inner_join(welfare_data, by=c("year", "month")) |>
    mutate(month = paste(year, month), month = parse_date(month, "%Y %m")) |>
    mutate(president=as.factor(ifelse(year(month) <= 2008, "Bush", ifelse(year(month) <= 2016, "Obama", "Trump"))))

trend_data_given |>
    pivot_longer(names_to = "trend_type", values_to = "trend_value", cols = ends_with("_trend")) |>
    mutate(trend_type=factor(trend_type, labels=c("report_trend", "crime_trend", "welfare_trend"))) |>
    ggplot(aes(x=month, y=trend_value, color=president)) +
    geom_point(alpha = 0.3) +
    geom_smooth(method=lm, formula = y ~ x, se=F) +
    facet_wrap(~ trend_type, dir="v")

table_3_data_given <- trend_data_given |>
    mutate(is_bush = president == "Bush", is_trump = president == "Trump")

list(
    crime = lm(crime_trend ~ month + is_bush + is_trump, data = table_3_data_given),
    welfare = lm(welfare_trend ~ month + is_bush + is_trump, data = table_3_data_given),
    report = lm(report_trend ~ month + is_bush + is_trump, data = table_3_data_given)
) |> msummary(output='markdown')
```
With this, we get the same data as he does. We still don't get quite the same regression, but we get closer. Interestingly, a lot of times, they have non-zero trends when we have zero trends.

But it's also true that he downloaded this in 3 separate files while we did it in a single file. Since Google measures things relative to each other when you download them together, it's possible that this changes things.

## Table 4 Using Replication Daily Data

Now we will replicate Table 4 using the given day values. Once again, the daily values used by the original paper were provided to us, and you can use it to replicate this.

```{r}

given_daily_data <- read_csv('from_replication_files/gt_report_daily.csv')

topic_table_daily <- document_topics |>
    group_by(date, time) |>
    summarize(proportion_1 = sum(Topic1), proportion_3 = sum(Topic3), crime_coverage = proportion_1 + proportion_3, welfare_coverage = sum(Topic13), num_segments = n()) |>
    mutate(trump_admin = time == "post-election", month_of_year=month(date, label=T), day_of_week=wday(date, label=T)) |>
    inner_join(given_daily_data, by="date")

list(
    search_model=lm(search ~ num_segments + crime_coverage + welfare_coverage + trump_admin + date + day_of_week + month_of_year, topic_table_daily),
    search_adj_model=lm(search_adj ~ num_segments + crime_coverage + welfare_coverage + trump_admin + date + day_of_week + month_of_year, topic_table_daily)
)|>
    msummary(output='markdown')
```
From here, we see that while neither of them are exactly identical, the one using the adjusted values is much closer to what they got. But the non-adjusted version finds a much weaker effect, with crime coverage having an ambiguous effect.

Looking at the data, it seems that the problem is that they can only look at one time period (say, month) at a time, and each month is only scaled with other searches in that month. But, I don't understand how their normalization works.

## Figure 4 and Table 3 Using Individually Downloaded Data

We noticed that they got each prompt separately. Since Google Trends weights everything you are comparing to the max of the other, if we want to replicate their results, then we need to get them separately like what the researchers seem to have done. We downloaded the [immigrant report](https://trends.google.com/trends/explore?date=2004-01-01%202019-12-31&geo=US&q=report%20immigrant%20%2B%20report%20immigration%20%2B%20report%20illegals%20%2B%20report%20illegal%20alien%20%2B%20report%20to%20ice&hl=en), [crime](https://trends.google.com/trends/explore?date=2004-01-01%202019-12-31&geo=US&q=immigrant%20crime%2Bimmigrant%20criminal%2Bimmigrant%20murder%2Bimmigrant%20kill&hl=en), and [welfare trends](https://trends.google.com/trends/explore?date=2004-01-01%202019-12-31&geo=US&q=immigrant%20welfare%2Bimmigrant%20cost%2Bimmigrant%20benefits&hl=en) separately.

```{r}
report_data <- read_csv('from_google_trends/report_trend_individual.csv', skip=3, col_names=c('month', 'report_trend'))
crime_data <- read_csv('from_google_trends/crime_trend_individual.csv', skip=3, col_names=c('month', 'crime_trend'))
welfare_data <- read_csv('from_google_trends/welfare_trend_individual.csv', skip=3, col_names=c('month', 'welfare_trend'))

trend_data_individual <- report_data |>
    inner_join(crime_data, by=c("month")) |>
    inner_join(welfare_data, by=c("month")) |>
    mutate(month = parse_date(month, "%Y-%m")) |>
    mutate(president=as.factor(ifelse(year(month) <= 2008, "Bush", ifelse(year(month) <= 2016, "Obama", "Trump"))))

trend_data_individual |>
    pivot_longer(names_to = "trend_type", values_to = "trend_value", cols = ends_with("_trend")) |>
    mutate(trend_type=factor(trend_type, labels=c("report_trend", "crime_trend", "welfare_trend"))) |>
    ggplot(aes(x=month, y=trend_value, color=president)) +
    geom_point(alpha = 0.3) +
    geom_smooth(method=lm, formula = y ~ x, se=F) +
    facet_wrap(~ trend_type, dir="v")

table_3_data_individual <- trend_data_individual |>
    mutate(is_bush = president == "Bush", is_trump = president == "Trump")

list(
    crime = lm(crime_trend ~ month + is_bush + is_trump, data = table_3_data_individual),
    welfare = lm(welfare_trend ~ month + is_bush + is_trump, data = table_3_data_individual),
    report = lm(report_trend ~ month + is_bush + is_trump, data = table_3_data_individual)
) |> msummary(output='markdown')
```
But the paper still has welfare searches going up in Trump's presidency, while we have it going down slightly. And in the table, they have welfare going down over time, while we have it going up. So, this discrepency doesn't explain anything.

# Extension

The paper says: "We also expect that when the Trump administration receives media coverage for anti-immigrant rhetoric or policies, we will see an uptick in reporting searches. However, we do not expect a discontinuity in  reporting searches immediately after the 2016 election, nor do we expect media coverage of candidate Trump’s anti-immigrant rhetoric to generate more reporting searches. Neither candidate Trump nor president-elect Trump had the power to change immigration policy before his inauguration, so his anti-immigrant positions should not increase interest in reporting" (5-6). Smriti and I are skeptical of this claim, which the paper doesn't seem to actually prove, so we are going to investigate it and see if we find any discontinuities, and whether coverage of Candidate Trump also increases the amount of reporting.

## Candidate Trump Boosting Reports

So, we will make a regression, where we have trump and candidacy stage as * along with month see if that boosts it.

```{r}
trump_extension_table <- document_topics |>
    mutate(month = floor_date(date, "month")) |>
    group_by(month, time, trump) |>
    summarize(proportion_1 = sum(Topic1), proportion_3 = sum(Topic3), crime_coverage = proportion_1 + proportion_3, welfare_coverage = sum(Topic13), num_segments = n()) |>
    mutate(month_of_year=month(month, label=T)) |>
    inner_join(trend_data_individual, by="month")

list(
    lm(report_trend ~ num_segments + crime_coverage + welfare_coverage + time * trump + month + month_of_year, trump_extension_table),
    lm(report_trend ~ num_segments + crime_coverage + welfare_coverage + time * trump + month, trump_extension_table)
     ) |>
    msummary(output='markdown')
```
If we regress by month in addition to our other variables, then we get what the paper predicts: trump's name causes a bigger boost post-election than the campaign. However, since the Trump boost (3.374) is more than 3, and the decrease in the campaign is less than that (-2.447), Trump *still* has a positive association with report searches, even in the campaign. When we stop filtering by month, this remains the case.

However, the standard error is so large that findings here may be meaningless.

## Discontinuity on Election

Now, we check if we can find a discontinuity in reporting search rates immediately after the election in the same way that the paper found one after inauguration. We look at the discontinuity at those two locations as well as two arbitrary locations - September 1st before the election and April 1st afterwards - to see if we can find similar discontinuities.

Since the data available to us isn't available in high-enough precision for this, we use their adjusted daily search data for more precise measurements.

```{r}
discontinuity_data <- given_daily_data |>
    mutate(arbitrary_before = date > ymd("2016-09-01"), after_election = date > ymd("2016-11-8"), after_inauguration = date > ymd("2017-01-20"), arbitrary_after = date > ymd("2017-04-01")) |>
    filter(date >= ymd("2014-01-01"))

discontinuity_data |>
    pivot_longer(c(arbitrary_before, after_election, after_inauguration, arbitrary_after), names_to = "split_location", values_to = "split_values") |>
    ggplot(aes(x=date, y=search_adj, color=split_values)) +
    geom_point(alpha = 0.05) +
    geom_smooth(method=lm, formula = y ~ x, se=F) +
    facet_wrap(~ split_location)
```

So, showing a bunch of splits, we can see that while regressing separately between before and after the election showed a large discontinuity, so did regressing before and after the election, which the paper claimed wouldn't have such a discontinuity. When we use the arbitrary splits, the one afterwards shows almost no discontinuity, but the split beforehand does show one, albeit not as large. Because of this, I don't think that showing a discontinuity in linear regressions has that precise of an effect and question its use in the paper.

We also wanted to see it as a regression so we could have a numerical measure of the discontinuity.

```{r}
list(
    lm(search_adj ~ date + arbitrary_before, discontinuity_data),
    lm(search_adj ~ date + after_election, discontinuity_data),
    lm(search_adj ~ date + after_inauguration, discontinuity_data),
    lm(search_adj ~ date + arbitrary_after, discontinuity_data)
) |>
    msummary(output="markdown")
```
Here, we see that while the largest discontinuity is, in fact, after the inauguration, at 23.374, the election split has almost as big of a modifier with 22.202 - in fact, since it's within the margin of error, we can't really say which one is actually better. While the earlier arbitrary time has a smaller discontinuity, at 16.113, it is still large.

The discontinuity does show an increase of searches for reporting immigrants during Trump's term. But the presence of a discontinuity does not mean that the exact location chose to split the data means anything.
